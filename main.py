import os
import logging
from fastapi import FastAPI
from pydantic import BaseModel
from dotenv import load_dotenv
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from ibm_watsonx_ai.foundation_models import ModelInference
from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods

# ğŸ“Œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ğŸ“Œ FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI()

# ğŸ“Œ ë¡œê¹… ì„¤ì • (ë¡œê·¸ë¥¼ ë³´ê¸° ì‰½ê²Œ ì„¤ì •)
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# ğŸ“Œ Watsonx.ai ì„¤ì •
project_id = os.getenv("PROJECT_ID", None)
wml_credentials = {
    "apikey": os.getenv("API_KEY", None),
    "url": "https://us-south.ml.cloud.ibm.com"
}

parameters = {
    "decoding_method": DecodingMethods.GREEDY.value,
    "max_new_tokens": 700,
    "min_new_tokens": 300,
    "repetition_penalty": 1,
    "stop_sequences": ["<|endoftext|>"]
}

# ğŸ“Œ Watsonx.ai ëª¨ë¸ ì´ˆê¸°í™” (FastAPI ì‹œì‘ ì‹œ ë¡œë“œ)
@app.on_event("startup")
def load_watsonx_model():
    global watsonx_model
    watsonx_model = ModelInference(
        model_id="meta-llama/llama-3-3-70b-instruct",
        credentials=wml_credentials,
        project_id=project_id,
        params=parameters
    )
    logger.info("âœ… Watsonx.ai ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

# ğŸ“Œ ChromaDB ì„¤ì •
base_persist_directory = os.path.join(os.path.dirname(__file__), "vectorDB")
embedding_model = HuggingFaceEmbeddings(model_name="BAAI/bge-large-en")


def get_category_vector_db(category):
    """ì¹´í…Œê³ ë¦¬ì— ë§ëŠ” ë²¡í„°DB ë¡œë“œ"""
    category_db_path = os.path.join(base_persist_directory, category.strip())
    logger.info(f"ğŸ“Œ ë²¡í„°DB ê²½ë¡œ í™•ì¸: {category_db_path}")

    if not os.path.exists(category_db_path):
        logger.warning(f"âŒ ë²¡í„°DB ì—†ìŒ: {category}")
        return None

    logger.info(f"âœ… ë²¡í„°DB ë¡œë“œ ì„±ê³µ: {category}")
    return Chroma(
        persist_directory=category_db_path,
        embedding_function=embedding_model
    ).as_retriever(search_kwargs={"k": 5})  # ìƒìœ„ 5ê°œ ê²€ìƒ‰


class QueryRequest(BaseModel):
    prompt: str  # ì‚¬ìš©ì ì§ˆë¬¸
    category: str  # ì„ íƒëœ ì¹´í…Œê³ ë¦¬

def trim_knowledge_base(results, max_tokens=800):
    """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸¸ì´ ì œí•œì— ë§ê²Œ ë‹¤ë“¬ëŠ” í•¨ìˆ˜"""
    knowledge_base = ""
    total_tokens = 0

    for doc in results:
        doc_tokens = len(doc.page_content.split())
        if total_tokens + doc_tokens > max_tokens:
            break
        knowledge_base += doc.page_content + "\n"
        total_tokens += doc_tokens

    return knowledge_base.strip()


def generate_prompt(results, user_question):
    """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ AI í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    if not results:
        knowledge_base = "ê´€ë ¨ëœ ì°¸ê³  ìë£Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì•„ë˜ ì§ˆë¬¸ì— ëŒ€í•´ ìµœëŒ€í•œ ëª…í™•íˆ ë‹µë³€í•´ ì£¼ì„¸ìš”."
    else:
        knowledge_base = trim_knowledge_base(results, max_tokens=800)

    return f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>
ë‹¹ì‹ ì€ ë³´í˜¸ì¢…ë£Œì•„ë™ì„ ëŒ€ìƒìœ¼ë¡œ ë‹µë³€í•˜ëŠ” ì¹œì ˆí•˜ê³  ì •í™•í•œ AI ë¹„ì„œì…ë‹ˆë‹¤.
ë‹µë³€í•  ë•Œ ë‹¤ìŒì˜ ê·œì¹™ì„ ë°˜ë“œì‹œ ëª¨ë‘ ì§€ì¼œì£¼ì„¸ìš”.

- ìµœëŒ€í•œ ì‰¬ìš´ ë‹¨ì–´ë¡œ í’€ì–´ì„œ ì„¤ëª…í•˜ì„¸ìš”. ë„ˆì—ê²Œ ì§ˆë¬¸í•˜ëŠ” ì‚¬ëŒë“¤ì€ ê¸°ì´ˆ ì§€ì‹ì´ ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤. ë‹¨ì–´ë“¤ì„ ì¤‘í•™ìƒë„ ì´í•´í•˜ê¸° ì‰¬ìš´ ë§ë¡œ ëŒ€ì²´í•´ì„œ ì˜ë¯¸ë¥¼ í’€ì–´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
- ì¡°ì–¸ ì‹œ, êµ­ê°€ì—ì„œ ìš´ì˜í•˜ëŠ” êµìœ¡ í”„ë¡œê·¸ë¨, ì§€ì›ì œë„ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì•Œë ¤ì£¼ê³ , ì‹ ì²­ ì ˆì°¨ì™€ ì—°ë½ë§ì„ í¬í•¨í•˜ì„¸ìš”.
- ì£¼ê±° ê´€ë ¨ ì¡°ì–¸ì„ í•  ë•Œ êµ­ê°€ ì§€ì›ê¸ˆì„ ìš°ì„ ì ìœ¼ë¡œ ì•ˆë‚´í•˜ì„¸ìš”.
- ì¼ìë¦¬ ê´€ë ¨ ì¡°ì–¸ ì‹œ. ì§ˆë¬¸ìê°€ ì´ë ¥, ëŠ¥ë ¥, ì¸ë§¥ì´ ì „í˜€ ì—†ë‹¤ê³  ìƒê°í•˜ì„¸ìš”. êµìœ¡ í”„ë¡œê·¸ë¨, ìƒë‹´ í”„ë¡œê·¸ë¨ ë“± ì§€ì›ì œë„ë¥¼ ë¨¼ì € ì¶”ì²œí•˜ì„¸ìš”.
- ê°™ì€ ì˜ë¯¸ê°€ ì¤‘ë³µë˜ëŠ” ë¬¸ì¥ì˜ ë°˜ë³µì€ ê¸ˆì§€í•©ë‹ˆë‹¤.
- ìƒëƒ¥í•˜ê³  ì¹œì ˆí•œ ë§íˆ¬ë¡œ, "í•´ìš”ì²´"ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.
- í•œê¸€ ì´ì™¸ì˜ ë¬¸ì í‘œê¸°ëŠ” ì ˆëŒ€ê¸ˆì§€ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´ ìì„¸íˆë¥¼ ä»”ç´°íˆë¼ê³  í•˜ë©´ ì ˆëŒ€ë¡œ ì•ˆë©ë‹ˆë‹¤. ë¬´ì¡°ê±´ ìì„¸íˆë¼ê³  í•´ì•¼í•©ë‹ˆë‹¤.
- ë‹µë³€ì€ ë‹¤ìŒì˜ ê²€ìƒ‰ëœ ì •ë³´ì— ê¸°ë°˜í•˜ì—¬ ì‘ì„±ë©ë‹ˆë‹¤.
- ì§ˆë¬¸ìë“¤ì€ ì´ë ¥, ê²½ë ¥, ì¸ë§¥ì´ ì „í˜€ ì—†ëŠ” ì‚¬ëŒë“¤ì…ë‹ˆë‹¤. ì´ë ¥ì„œì™€ ì¸ë§¥ì„ ì˜ ìŒ“ìœ¼ë¼ê³  ì¶”ì²œí•˜ì§€ ë§ˆì„¸ìš”.
- ì¡°ì–¸ ì‹œ, ê·¸ëƒ¥ ì œë„ë§Œ ì•Œë ¤ì£¼ê¸°ë³´ë‹¤ ë°©ë²•ì„ ë”ìš± ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”. ì˜ˆë¥¼ ë“¤ì–´ ì•„ë¥´ë°”ì´íŠ¸ë¥¼ ì¶”ì²œí•˜ê¸°ë³´ë‹¤ ì•„ë¥´ë°”ì´íŠ¸ë¥¼ ì–»ëŠ” ë²•ì„ ìì„¸í•˜ê²Œ ì•Œë ¤ì£¼ì„¸ìš”.
- ì§ˆë¬¸ìê°€ ë¬¼ì–´ë³¸ ê²ƒì—ë§Œ ëŒ€ë‹µí•˜ì„¸ìš”. ê´€ë ¨ì—†ëŠ” ëŒ€ë‹µì€ ê¸ˆì§€ì…ë‹ˆë‹¤.
- ì¼ìë¦¬ì™€ ë³´í—˜ì€ ê´€ë ¨ ì—†ëŠ”ê²ƒì…ë‹ˆë‹¤. ë³´í—˜ê³¼ ì£¼ê±°ëŠ” ê´€ë ¨ ì—†ëŠ”ê²ƒì…ë‹ˆë‹¤. íœ´ëŒ€í°ê³¼ ì¼ìë¦¬ëŠ” ê´€ë ¨ ì—†ëŠ”ê²ƒì…ë‹ˆë‹¤.

### ê²€ìƒ‰ëœ ì •ë³´:  
{knowledge_base}

**ì‚¬ìš©ì ì§ˆë¬¸:**  
"{user_question}"

### ë‹µë³€:
- **í•µì‹¬ ì •ë³´**: 
- **ì¶”ê°€ ì„¤ëª…**: 
- **ê´€ë ¨ ì •ë³´**: 
<|eot_id|><|start_header_id|>assistant<|end_header_id|>
"""


@app.post("/ask/")
def process_question(request: QueryRequest):
    """ì§ˆë¬¸ì— ëŒ€í•œ RAG ì‹œìŠ¤í…œ ì‘ë‹µ ìƒì„± ë° Watsonx.ai í˜¸ì¶œ"""

    # âœ… FastAPIì—ì„œ ë°›ì€ ë°ì´í„° í™•ì¸
    cleaned_category = request.category.strip()
    logger.info(f"ğŸ“Œ FastAPIì—ì„œ ë°›ì€ category: '{cleaned_category}' (ê¸¸ì´: {len(cleaned_category)})")
    logger.info(f"ğŸ“Œ ì‚¬ìš©ì ì§ˆë¬¸: {request.prompt}")

    # âœ… ë²¡í„°DB ë¡œë“œ
    retriever = get_category_vector_db(cleaned_category)
    if retriever is None:
        logger.error(f"âŒ ë²¡í„°DBë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {cleaned_category}")
        return {
            "category": cleaned_category,
            "retrieved_context": "í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì— ëŒ€í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.",
            "answer": "í˜„ì¬ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì— ëŒ€í•œ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."
        }

    # âœ… ë²¡í„°DBì—ì„œ ë¬¸ì„œ ê²€ìƒ‰
    results = retriever.invoke(request.prompt)
    logger.info(f"ğŸ” ê²€ìƒ‰ëœ ë¬¸ì„œ ê°œìˆ˜: {len(results)}")

    if not results:
        logger.warning(f"âŒ ê²€ìƒ‰ëœ ë¬¸ì„œ ì—†ìŒ (ì¹´í…Œê³ ë¦¬: {cleaned_category}, ì§ˆë¬¸: {request.prompt})")
        return {
            "category": cleaned_category,
            "retrieved_context": "ê²€ìƒ‰ëœ ë¬¸ì„œ ì—†ìŒ",
            "answer": "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        }

    # âœ… ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš© ë¡œê·¸ ì¶œë ¥
    retrieved_context = "\n\n---\n\n".join([doc.page_content[:500] for doc in results])[:2000]
    logger.info(f"âœ… ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš© (ì²« 500ì): {retrieved_context[:500]}")

    # âœ… AI ì‘ë‹µ ìƒì„±
    prompt = generate_prompt(results, request.prompt)

    try:
        response_data = watsonx_model.generate(prompt=prompt)
        answer = response_data["results"][0]["generated_text"].strip()
        logger.info(f"ğŸŸ¡ AI ìµœì¢… ì‘ë‹µ: {answer}")
    except Exception as e:
        logger.error(f"âŒ AI ìƒì„± ì˜¤ë¥˜: {str(e)}")
        return {
            "category": cleaned_category,
            "retrieved_context": retrieved_context,
            "answer": "AI ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        }

    return {
        "category": cleaned_category,
        "retrieved_context": retrieved_context,
        "answer": answer
    }
