from fastapi import FastAPI
from pydantic import BaseModel
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from ibm_watsonx_ai.foundation_models import ModelInference
from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods
from dotenv import load_dotenv
import os

# ğŸ“Œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ğŸ“Œ FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI()

# ğŸ“Œ Watsonx.ai ì„¤ì •
project_id = os.getenv("PROJECT_ID", None)
wml_credentials = {
    "apikey": os.getenv("API_KEY", None),
    "url": "https://us-south.ml.cloud.ibm.com"
}

parameters = {
    "decoding_method": DecodingMethods.GREEDY.value,
    "min_new_tokens": 1,
    "max_new_tokens": 1000,
    "stop_sequences": ["<|endoftext|>"]
}

# Watsonx.ai ëª¨ë¸ ì´ˆê¸°í™” (ìµœì‹  ëª¨ë¸ ì‚¬ìš©)
watsonx_model = ModelInference(
    model_id="meta-llama/llama-3-3-70b-instruct",
    credentials=wml_credentials,
    project_id=project_id,
    params=parameters
)

# ChromaDB ì´ˆê¸°í™” (ë²¡í„°DB ì„¤ì •)
persist_directory = "/home/ibmuser01/team5_beta/chromadb_sqlite"
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vector_db = Chroma(persist_directory=persist_directory, embedding_function=embedding_model)
retriever = vector_db.as_retriever(search_kwargs={"k": 3}) 

# ìš”ì²­ ë°ì´í„° ëª¨ë¸ ì •ì˜
class QueryRequest(BaseModel):
    question: str
    user_system_prompt: str = ""  
    max_tokens: int = 1000  
    gender: str
    age: int
    category: str
    
# ================================ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ================================ #

def generate_prompt(results, user_question):
    """ ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í¬í•¨í•œ ìµœì¢… Watsonx.ai í”„ë¡¬í”„íŠ¸ ìƒì„± """

    knowledge_base = "\n".join(doc.page_content for doc in results)  # ê²€ìƒ‰ëœ ë¬¸ì„œ ì¶”ê°€

    prompt = f"""
ë‹¹ì‹ ì€ ì •ì§í•œ AI ë¹„ì„œì´ì, ë‹¹ì‹ ì˜ ê³ ê°ì€ ë³´ìœ¡ì›ì—ì„œ ë‚˜ì˜¨ ë³´í˜¸ì¢…ë£Œì•„ë™ì´ì—ìš”.
ì²­ì†Œë…„ë“¤ì—ê²Œ ì„¤ëª…í•˜ëŠ” ê²ƒì¸ ë§Œí¼, **ë§¤ìš° ì¹œì ˆí•˜ê³  ìƒëƒ¥í•˜ê²Œ**, ê·¸ë¦¬ê³  **êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€**í•´ì£¼ì„¸ìš”.

ğŸ”¹ **ì§€ì‹œì‚¬í•­**:
- ë°˜ë“œì‹œ ì•„ë˜ **knowledge base**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
- **ì¶”ê°€ì ì¸ ê°€ì •ì´ë‚˜ ì¶”ì¸¡**ì„ í•˜ì§€ ë§ˆì„¸ìš”.
- ë¬¸ë‹¨ ì‚¬ì´ì—ëŠ” **ê³µë°±ì„ ì¶”ê°€**í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì´ì„¸ìš”.

ğŸ” **ë‹¤ìŒì€ ì°¸ê³ í•´ì•¼ í•  knowledge baseì…ë‹ˆë‹¤.**:
{knowledge_base}

---
ğŸ“Œ **ì‘ë‹µ í˜•ì‹**:
1. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ **ìƒëƒ¥í•˜ê³  êµ¬ì²´ì ì´ê³  ì¹œì ˆí•œ ë‹µë³€**ì„ ì œê³µí•˜ê³ , í•´ìš”ì²´ë¥¼ ì“°ì„¸ìš”.
2. **ì¶”ê°€ë¡œ ë„ì›€ì´ ë  ë§Œí•œ ê´€ë ¨ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ **3~5ê°œ** ìƒì„±í•˜ì„¸ìš”.
3. ê°™ì€ ë‚´ìš©ì˜ ì§ˆë¬¸ì´ ë°˜ë³µë˜ì§€ ì•Šë„ë¡ **ì£¼ì˜í•˜ì„¸ìš”**.
4. ë‹¤ìŒ í˜•ì‹ì„ ë”°ë¥´ì„¸ìš”:
---
ğŸ’¬ **ë©”ì¸ ë‹µë³€**:
[ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ë‹µë³€]

=========================================
â“ ì§ˆë¬¸: [ê´€ë ¨ ì§ˆë¬¸ 1]
ğŸ’¬ ë‹µë³€: [ê´€ë ¨ ì§ˆë¬¸ 1ì— ëŒ€í•œ ë‹µë³€]

â“ ì§ˆë¬¸: [ê´€ë ¨ ì§ˆë¬¸ 2]
ğŸ’¬ ë‹µë³€: [ê´€ë ¨ ì§ˆë¬¸ 2ì— ëŒ€í•œ ë‹µë³€]


---
â“ **ì‚¬ìš©ìì˜ ì§ˆë¬¸**: {user_question}
ğŸ’¬ **ë‹µë³€**:
"""
    return prompt


@app.post("/ask/")
def process_question(request: QueryRequest):
    """RAG ì‹œìŠ¤í…œì„ í†µí•´ ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µì„ ìƒì„±"""

    print(f"\nğŸ“Œ [DEBUG] ì‚¬ìš©ì ì§ˆë¬¸: {request.question}")

    retriever_input = (
        f"{request.age}ì„¸ {request.gender} "
        f"{request.category} : {request.question}"
    )

    print(f"\nğŸ” [DEBUG] retrieverì— ì…ë ¥ëœ ìµœì¢… ê²€ìƒ‰ í‚¤ì›Œë“œ:\n{retriever_input}\n{'='*50}")

    results = retriever.get_relevant_documents(retriever_input)

    print("\nğŸ” [DEBUG] ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œ ëª©ë¡:")
    for idx, doc in enumerate(results, start=1):
        source = doc.metadata.get("source", "ì¶œì²˜ ì—†ìŒ")
        content_preview = doc.page_content[:300]  # ë¬¸ì„œ ë‚´ìš© ì• 300ì ë¯¸ë¦¬ë³´ê¸°
        print(f"\nğŸ“„ ë¬¸ì„œ {idx}: {source}\nğŸ“œ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°:\n{content_preview}\n{'-'*50}")

    retrieved_docs = [{"source": doc.metadata.get("source", "ì¶œì²˜ ì—†ìŒ"), "content": doc.page_content[:300]} for doc in results]

    # Watsonx.aiì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = generate_prompt(results, request.question)

    print(f"\nğŸš€ [DEBUG] Watsonx.aiì— ì „ë‹¬ë  ìµœì¢… í”„ë¡¬í”„íŠ¸:\n{prompt}\n{'='*80}")

    try:
        response_data = watsonx_model.generate(prompt=prompt)
        answer = response_data["results"][0]["generated_text"].strip()
        
        # ë””ë²„ê¹…ìš©: Watsonx.ai ì‘ë‹µ ì¶œë ¥
        print(f"\nâœ… [DEBUG] LLMì´ ìƒì„±í•œ ë‹µë³€:\n{answer}\n{'='*80}")

    except Exception as e:
        answer = f"âš ï¸ Watsonx.ai ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    # ê¸°ë³¸ ì•ˆë‚´ ë¬¸êµ¬ ì¶”ê°€
    answer += """
=========================================
ğŸ“ ê´€ë ¨ ê¸°ê´€ ë¬¸ì˜: 123-456-7890\nğŸŒ í™ˆí˜ì´ì§€: www.example.com"
ğŸ’• ë‹¹ì‹ ì˜ í˜ì°¬ ë‚´ì¼ì„ ì‘ì›í•©ë‹ˆë‹¤ ğŸ’•
"""
    # ë””ë²„ê¹…ìš©: ìµœì¢… ì‘ë‹µ ì¶œë ¥
    print(f"\nâœ… [DEBUG] Streamlitìœ¼ë¡œ ì „ë‹¬ë  ìµœì¢… ì‘ë‹µ:\n{answer}\n{'='*80}")

    return {"question": request.question, "answer": answer, "retrieved_docs": retrieved_docs}