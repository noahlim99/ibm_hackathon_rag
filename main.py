from fastapi import FastAPI
from pydantic import BaseModel
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from ibm_watsonx_ai.foundation_models import ModelInference
from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods
from dotenv import load_dotenv
import os
import logging

# ğŸ“Œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ğŸ“Œ FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI()

# ğŸ“Œ ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ğŸ“Œ Watsonx.ai ì„¤ì •
project_id = os.getenv("PROJECT_ID", None)
wml_credentials = {
    "apikey": os.getenv("API_KEY", None),
    "url": "https://us-south.ml.cloud.ibm.com"
}

parameters = {
    "decoding_method": DecodingMethods.SAMPLE.value,
    "temperature": 0.5,
    "min_new_tokens": 1,
    "max_new_tokens": 1000,
    "stop_sequences": ["<|endoftext|>"]
}

# Watsonx.ai ëª¨ë¸ ì´ˆê¸°í™” (ìµœì‹  ëª¨ë¸ ì‚¬ìš©)
watsonx_model = ModelInference(
    model_id="meta-llama/llama-3-3-70b-instruct",
    credentials=wml_credentials,
    project_id=project_id,
    params=parameters
)

# ChromaDB ë””ë ‰í† ë¦¬ ê¸°ë³¸ ê²½ë¡œ
base_persist_directory = os.path.join(os.path.dirname(__file__), "vectorDB")
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# ğŸ“Œ ìš”ì²­ ë°ì´í„° ëª¨ë¸
class QueryRequest(BaseModel):
    prompt: str  # ì§ˆë¬¸ í…ìŠ¤íŠ¸ë¥¼ ì €ì¥í•˜ëŠ” í•„ë“œ
    category: str  # ì„ íƒëœ ì¹´í…Œê³ ë¦¬

# ================================ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ================================ #

def trim_knowledge_base(results, max_tokens=700):
    """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸¸ì´ ì œí•œì— ë§ê²Œ ë‹¤ë“¬ëŠ” í•¨ìˆ˜"""
    knowledge_base = ""
    total_tokens = 0

    for doc in results:
        doc_tokens = len(doc.page_content.split())  # í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ ê¸°ì¤€ìœ¼ë¡œ í† í°í™”
        if total_tokens + doc_tokens > max_tokens:
            # ì´ˆê³¼ë˜ëŠ” ê²½ìš°, í•„ìš”í•œ ë§Œí¼ë§Œ ì˜ë¼ì„œ ì¶”ê°€
            remaining_tokens = max_tokens - total_tokens
            knowledge_base += " ".join(doc.page_content.split()[:remaining_tokens]) + "\n"
            break
        knowledge_base += doc.page_content + "\n"
        total_tokens += doc_tokens

    return knowledge_base.strip()

def generate_prompt(results, user_question):
    """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í¬í•¨í•œ ìµœì¢… Watsonx.ai í”„ë¡¬í”„íŠ¸ ìƒì„±"""

    if not results:
        knowledge_base = "ê´€ë ¨ëœ ì°¸ê³  ìë£Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì•„ë˜ ì§ˆë¬¸ì— ëŒ€í•´ ìµœëŒ€í•œ ëª…í™•íˆ ë‹µë³€í•´ ì£¼ì„¸ìš”."
    else:
        # 700í† í° ì œí•œ ì ìš©
        knowledge_base = trim_knowledge_base(results, max_tokens=700)

    prompt = f"""
ë‹¹ì‹ ì€ ë³´í˜¸ì¢…ë£Œì•„ë™ì„ ë•ëŠ” **ì •ì§í•˜ê³  ì¹œì ˆí•œ AI ë¹„ì„œ**ì…ë‹ˆë‹¤.  
ì²­ì†Œë…„ì„ ìƒëŒ€í•˜ëŠ” ë§Œí¼, **ê²½ì¾Œí•˜ê³  ì‹ ë‚˜ëŠ” ë§íˆ¬**ë¡œ ë§í•˜ê³ , ë¶ˆí•„ìš”í•œ ë°˜ë³µ ì—†ì´ í•µì‹¬ë§Œ ì „ë‹¬í•˜ì„¸ìš”. 

ğŸ”¹ **[ì‘ë‹µ ì§€ì¹¨]**  
1. ë°˜ë“œì‹œ ì•„ë˜ ì œê³µëœ **knowledge base**ì˜ ì •ë³´ë§Œ í™œìš©í•˜ì„¸ìš”.  
2. **ë¶ˆí•„ìš”í•œ ë°˜ë³µ ì—†ì´ í•µì‹¬ ì •ë³´ë§Œ ìš”ì•½í•˜ì—¬ ì„¤ëª…**í•˜ì„¸ìš”.  
3. **ì´í•´í•˜ê¸° ì‰¬ìš´ ë¬¸ì¥**ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ê³ , **ì‚¬ì¡±ì„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.**  
4. ì‘ë‹µì€ ë°˜ë“œì‹œ "**í•´ìš”ì²´**"ë¡œ, ë§¤ìš° ì¹œì ˆí•˜ê³  ìƒëƒ¥í•œ ë§íˆ¬ë¡œ ì‘ì„±í•˜ì„¸ìš”.  
5. **ì‘ë‹µì€ 700ì ì´í•˜**ë¡œ, ì¤‘ìš”í•œ í•µì‹¬ ì •ë³´ë§Œ ë‹´ê³ , ë§ˆì§€ë§‰ì— ë§ì´ ì¤‘ê°„ì— ëŠê¸°ì§€ ì•Šë„ë¡ ë‹¤ë“¬ìœ¼ì„¸ìš”.
6. **ë¶ˆí•„ìš”í•œ ê°íƒ„ì‚¬ ë°˜ë³µ ë° ì¸ì‚¬ë§ì€ ì ˆëŒ€ ê¸ˆì§€**í•©ë‹ˆë‹¤.  

---
ğŸ“Œ **[ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•  ì‘ë‹µ í˜•ì‹]**  
ğŸ’¡ **ì•„ë˜ì˜ í˜•ì‹ ê·¸ëŒ€ë¡œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.**  

âœ… **[ì‘ë‹µ ì˜ˆì‹œ]**  
**ì§ˆë¬¸:** "ë³´í˜¸ì¢…ë£Œì•„ë™ì´ ì§€ì›ë°›ì„ ìˆ˜ ìˆëŠ” í˜œíƒì´ ë­ì•¼?"  

**ë‹µë³€:**  
ë³´í˜¸ì¢…ë£Œì•„ë™ì´ ë°›ì„ ìˆ˜ ìˆëŠ” ì§€ì› í˜œíƒì— ëŒ€í•´ ì•Œë ¤ë“œë¦´ê²Œìš”!!

ğŸ“Œ **1. ìë¦½ì •ì°©ê¸ˆ**  
- ì¼ì • ê¸ˆì•¡ì„ ì§€ê¸‰ë°›ì•„ ì•ˆì •ì ìœ¼ë¡œ ìƒí™œì„ ì‹œì‘í•  ìˆ˜ ìˆì–´ìš”!  

ğŸ“Œ **2. ì£¼ê±° ì§€ì›**  
- LH ê³µê³µì„ëŒ€ì£¼íƒ ìš°ì„  ì…ì£¼, ì›”ì„¸ ì§€ì› ë“±ì´ ê°€ëŠ¥í•´ìš”!  

ğŸ“Œ **3. ì·¨ì—… ë° í•™ì—… ì§€ì›**  
- ì§ì—…í›ˆë ¨, ëŒ€í•™ ë“±ë¡ê¸ˆ ì§€ì› ë“± ë‹¤ì–‘í•œ í˜œíƒì´ ì œê³µë¼ìš”!

ë” ê¶ê¸ˆí•œ ì ì´ ìˆë‹¤ë©´ ë¬¼ì–´ë´ì£¼ì„¸ìš” ğŸ˜Š 
---
ğŸ” **ë‹¤ìŒì€ ì°¸ê³ í•´ì•¼ í•  knowledge baseì…ë‹ˆë‹¤.**  
{knowledge_base}  

ğŸ’¡ **[ì´ì œ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”]**  
**ì§ˆë¬¸:** {user_question}  

**ë‹µë³€:**
"""
    return prompt


# ================================ í›„ì²˜ë¦¬ í•¨ìˆ˜ ================================ #

def remove_redundant_phrases(answer):
    """ì¤‘ë³µëœ ë¬¸êµ¬ ë° ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ í‘œí˜„ ì œê±°"""
    lines = answer.split("\n")
    unique_lines = []
    seen_phrases = set()

    for line in lines:
        cleaned_line = line.strip()

        # ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë¬¸ì¥ì´ ë°˜ë³µë  ê²½ìš° ì œê±°
        if any(cleaned_line in seen for seen in seen_phrases) or cleaned_line in unique_lines:
            continue

        unique_lines.append(cleaned_line)
        seen_phrases.add(cleaned_line[:20])  # ë¬¸ì¥ì˜ ì• 20ìë§Œ ì €ì¥í•˜ì—¬ ì¤‘ë³µ ê°ì§€ ê°•í™”

    return "\n".join(unique_lines)


def fetch_full_answer(prompt):
    """Watsonx.aië¡œë¶€í„° ì‘ë‹µì„ ë°›ì•„ ì¤‘ë³µ ì œê±° í›„ ìµœì í™”ëœ ë‹µë³€ ìƒì„±"""
    full_answer = ""
    max_iterations = 4  # ë°˜ë³µ íšŸìˆ˜ë¥¼ ëŠ˜ë ¤ ë” ê¸´ ì‘ë‹µì„ ìƒì„±
    iteration = 0
    seen_sentences = set()  # ì¤‘ë³µ ê°ì§€ìš© ì§‘í•©
    last_answer = ""

    while iteration < max_iterations:
        response_data = watsonx_model.generate(prompt=prompt)
        partial_answer = response_data["results"][0]["generated_text"].strip()

        # ì‘ë‹µì´ ì—†ê±°ë‚˜, ë„ˆë¬´ ì§§ì„ ê²½ìš° ì¶”ê°€ í˜¸ì¶œ
        if not partial_answer or len(partial_answer) < 50:
            iteration += 1
            continue

        # ë™ì¼í•œ ì‘ë‹µ ë°˜ë³µ ë°©ì§€
        if partial_answer == last_answer:
            iteration += 1
            continue
        last_answer = partial_answer

        # ì¤‘ë³µ ì œê±°
        sentences = partial_answer.split("\n")
        filtered_sentences = [s for s in sentences if s.strip() and s.strip() not in seen_sentences]
        seen_sentences.update(filtered_sentences)

        full_answer += "\n".join(filtered_sentences) + "\n"

        # ì‘ë‹µ ê¸¸ì´ ì œí•œ & ì¢…ë£Œ ì¡°ê±´ í™•ì¸
        if len(full_answer) > 700 or any(s.endswith(("ğŸ˜Š", ".", "!", "?")) for s in filtered_sentences):
            break

        iteration += 1

    return remove_redundant_phrases(full_answer)


# ================================ FastAPI ì—”ë“œí¬ì¸íŠ¸ ================================ #

@app.post("/ask/")
def process_question(request: QueryRequest):
    """ì§ˆë¬¸ì— ëŒ€í•œ RAG ì‹œìŠ¤í…œ ì‘ë‹µ ìƒì„±"""
    category = request.category
    user_question = request.prompt
    logger.info(f"ğŸ“Œ [DEBUG] ì¹´í…Œê³ ë¦¬: {category}")
    logger.info(f"ğŸ“Œ [DEBUG] ì‚¬ìš©ì ì§ˆë¬¸: {user_question}")

    # ì¹´í…Œê³ ë¦¬ ë””ë ‰í† ë¦¬ í™•ì¸
    category_directory = os.path.join(base_persist_directory, category)
    if not os.path.exists(category_directory):
        return {"error": f"ì¹´í…Œê³ ë¦¬ '{category}'ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}

    # ë²¡í„°DB ë¡œë“œ ë° ê²€ìƒ‰
    vector_db = Chroma(persist_directory=category_directory, embedding_function=embedding_model)
    results = vector_db.similarity_search(user_question, k=5)
    if not results:
        return {"error": "ê²€ìƒ‰ëœ ê´€ë ¨ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”!"}

    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = generate_prompt(results, user_question)
    logger.info(f"\nğŸš€ [DEBUG] Watsonx.aiì— ì „ë‹¬ë  í”„ë¡¬í”„íŠ¸:\n{prompt}\n{'='*80}")

    # Watsonx.ai í˜¸ì¶œ ë° ê²°ê³¼ ìˆ˜ì§‘
    try:
        answer = fetch_full_answer(prompt)  # ì™„ì „í•œ ì‘ë‹µ ìˆ˜ì§‘
        logger.info(f"\nâœ… [DEBUG] ìµœì¢… ë‹µë³€:\n{answer}\n{'='*80}")
    except Exception as e:
        logger.error(f"Watsonx.ai ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return {"error": f"Watsonx.ai ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}

    # ìµœì¢… ì‘ë‹µ ë°˜í™˜
    return {"answer": answer} 