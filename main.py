from fastapi import FastAPI
from pydantic import BaseModel
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from ibm_watsonx_ai.foundation_models import ModelInference
from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods
from dotenv import load_dotenv
import os
import logging

# ğŸ“Œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ğŸ“Œ FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI()

# ğŸ“Œ ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ğŸ“Œ Watsonx.ai ì„¤ì •
project_id = os.getenv("PROJECT_ID", None)
wml_credentials = {
    "apikey": os.getenv("API_KEY", None),
    "url": "https://us-south.ml.cloud.ibm.com"
}

parameters = {
    "decoding_method": DecodingMethods.SAMPLE.value,
    "temperature": 0.5,
    "min_new_tokens": 1,
    "max_new_tokens": 1000,
    "stop_sequences": ["<|endoftext|>"]
}

# ğŸ“Œ Watsonx.ai ëª¨ë¸ ì´ˆê¸°í™” (ìµœì‹  ëª¨ë¸ ì‚¬ìš©)
watsonx_model = ModelInference(
    model_id="meta-llama/llama-3-3-70b-instruct",
    credentials=wml_credentials,
    project_id=project_id,
    params=parameters
)

# ğŸ“Œ ChromaDB ë””ë ‰í† ë¦¬ ê¸°ë³¸ ê²½ë¡œ
base_persist_directory = os.path.join(os.path.dirname(__file__), "vectorDB")
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# ğŸ“Œ ìš”ì²­ ë°ì´í„° ëª¨ë¸
class QueryRequest(BaseModel):
    prompt: str  # ì§ˆë¬¸ í…ìŠ¤íŠ¸ë¥¼ ì €ì¥í•˜ëŠ” í•„ë“œ
    category: str  # ì„ íƒëœ ì¹´í…Œê³ ë¦¬

# ================================ ìƒˆë¡œìš´ Retriever êµ¬í˜„ ================================ #

def retrieve_documents(user_question, category, top_k=5):
    """
    ì‚¬ìš©ìê°€ ì„ íƒí•œ ì¹´í…Œê³ ë¦¬ ë‚´ì—ì„œ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜
    """
    category_directory = os.path.join(base_persist_directory, category)
    
    # ğŸ”º ì¹´í…Œê³ ë¦¬ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ì˜¤ë¥˜ ë°˜í™˜
    if not os.path.exists(category_directory):
        return {"error": f"ì¹´í…Œê³ ë¦¬ '{category}'ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}
    
    # ğŸ”¹ ë²¡í„° DB ë¡œë“œ (ì„ë² ë”© ëª¨ë¸ ì ìš©)
    vector_db = Chroma(
        persist_directory=category_directory,
        embedding_function=embedding_model  # ğŸ”º ì„ë² ë”© ì ìš©
    )

    # ğŸ” ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰ (ìµœëŒ€ top_kê°œ)
    results = vector_db.similarity_search(user_question, k=top_k)

    # ğŸ”º ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ì˜¤ë¥˜ ë°˜í™˜
    if not results:
        return {"error": "ê²€ìƒ‰ëœ ê´€ë ¨ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”!"}

    return results  # ğŸ”¹ ì˜¬ë°”ë¥´ê²Œ ìœ ì‚¬ ë¬¸ì„œë§Œ ë°˜í™˜!


# ================================ Watsonx.ai í”„ë¡¬í”„íŠ¸ ìƒì„± ================================ #

def generate_prompt(results, user_question):
    """
    ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í¬í•¨í•˜ì—¬ Watsonx.aiì— ì „ë‹¬í•  ìµœì¢… í”„ë¡¬í”„íŠ¸ ìƒì„±
    """

    # ğŸ“Œ ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
    knowledge_base = "\n\n".join([doc.page_content for doc in results])

    prompt = f"""
ë‹¹ì‹ ì€ ë³´í˜¸ì¢…ë£Œì•„ë™ì„ ë•ëŠ” **ì •ì§í•˜ê³  ì¹œì ˆí•œ AI ë¹„ì„œ**ì…ë‹ˆë‹¤.  
ì²­ì†Œë…„ì„ ìƒëŒ€í•˜ëŠ” ë§Œí¼, **ê²½ì¾Œí•˜ê³  ì‹ ë‚˜ëŠ” ë§íˆ¬**ë¡œ ë§í•˜ê³ , ë¶ˆí•„ìš”í•œ ë°˜ë³µ ì—†ì´ í•µì‹¬ë§Œ ì „ë‹¬í•˜ì„¸ìš”. 

ğŸ”¹ **[ì‘ë‹µ ì§€ì¹¨]**  
1. ë°˜ë“œì‹œ ì•„ë˜ ì œê³µëœ **knowledge base**ì˜ ì •ë³´ë§Œ í™œìš©í•˜ì„¸ìš”.  
2. **ë¶ˆí•„ìš”í•œ ë°˜ë³µ ì—†ì´ í•µì‹¬ ì •ë³´ë§Œ ìš”ì•½í•˜ì—¬ ì„¤ëª…**í•˜ì„¸ìš”.  
3. **ì´í•´í•˜ê¸° ì‰¬ìš´ ë¬¸ì¥**ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ê³ , **ì‚¬ì¡±ì„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.**  
4. ì‘ë‹µì€ ë°˜ë“œì‹œ "**í•´ìš”ì²´**"ë¡œ, ë§¤ìš° ì¹œì ˆí•˜ê³  ìƒëƒ¥í•œ ë§íˆ¬ë¡œ ì‘ì„±í•˜ì„¸ìš”.  
5. **ì‘ë‹µì€ 700ì ì´í•˜**ë¡œ, ì¤‘ìš”í•œ í•µì‹¬ ì •ë³´ë§Œ ë‹´ê³ , ë§ˆì§€ë§‰ì— ë§ì´ ì¤‘ê°„ì— ëŠê¸°ì§€ ì•Šë„ë¡ ë‹¤ë“¬ìœ¼ì„¸ìš”.
6. **ë¶ˆí•„ìš”í•œ ê°íƒ„ì‚¬ ë°˜ë³µ ë° ì¸ì‚¬ë§ì€ ì ˆëŒ€ ê¸ˆì§€**í•©ë‹ˆë‹¤.  

---
ğŸ” **ë‹¤ìŒì€ ì°¸ê³ í•´ì•¼ í•  knowledge baseì…ë‹ˆë‹¤.**  
{knowledge_base}  

ğŸ’¡ **[ì´ì œ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”]**  
**ì§ˆë¬¸:** {user_question}  

**ë‹µë³€:**
"""
    return prompt

# ================================ FastAPI ì—”ë“œí¬ì¸íŠ¸ ================================ #

@app.post("/ask/")
def process_question(request: QueryRequest):
    """ì§ˆë¬¸ì— ëŒ€í•œ RAG ì‹œìŠ¤í…œ ì‘ë‹µ ìƒì„±"""
    category = request.category
    user_question = request.prompt
    logger.info(f"ğŸ“Œ [DEBUG] ì¹´í…Œê³ ë¦¬: {category}")
    logger.info(f"ğŸ“Œ [DEBUG] ì‚¬ìš©ì ì§ˆë¬¸: {user_question}")

    # ğŸ“Œ ìƒˆë¡œìš´ Retrieverë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ê²€ìƒ‰ (ì´ì œ ì „ì²´ ë¬¸ì„œê°€ ì•„ë‹ˆë¼ ìœ ì‚¬ ë¬¸ì„œë§Œ ê²€ìƒ‰!)
    results = retrieve_documents(user_question, category, top_k=5)

    # ğŸ”º ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ì„ ê²½ìš° ì—ëŸ¬ ë°˜í™˜
    if isinstance(results, dict) and "error" in results:
        return results

    # ğŸ“Œ Watsonx.ai í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = generate_prompt(results, user_question)
    logger.info(f"\nğŸš€ [DEBUG] Watsonx.aiì— ì „ë‹¬ë  í”„ë¡¬í”„íŠ¸:\n{prompt}\n{'='*80}")

    # ğŸ“Œ Watsonx.ai í˜¸ì¶œ ë° ê²°ê³¼ ìˆ˜ì§‘
    try:
        answer = watsonx_model.generate(prompt=prompt)["results"][0]["generated_text"].strip()
        logger.info(f"\nâœ… [DEBUG] ìµœì¢… ë‹µë³€:\n{answer}\n{'='*80}")
    except Exception as e:
        logger.error(f"Watsonx.ai ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return {"error": f"Watsonx.ai ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}

    # ğŸ“Œ ìµœì¢… ì‘ë‹µ ë°˜í™˜
    return {"answer": answer}
