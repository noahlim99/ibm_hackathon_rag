from fastapi import FastAPI
from pydantic import BaseModel
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from ibm_watsonx_ai.foundation_models import ModelInference
from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods
from dotenv import load_dotenv
import os
import logging

# ğŸ“Œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ğŸ“Œ FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI()

# ğŸ“Œ ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ğŸ“Œ Watsonx.ai ì„¤ì •
project_id = os.getenv("PROJECT_ID", None)
wml_credentials = {
    "apikey": os.getenv("API_KEY", None),
    "url": "https://us-south.ml.cloud.ibm.com"
}

parameters = {
    "decoding_method": DecodingMethods.SAMPLE.value,  # Sampling ë°©ì‹
    "temperature": 0.3,  # ì˜¨ë„
    "top_p": 0.3,  # ìƒìœ„ P (í•µì‹¬ ìƒ˜í”Œë§)
    "top_k": 10,  # ìƒìœ„ K
    "repetition_penalty": 1.5,  # ë°˜ë³µ í˜ë„í‹°
    "min_new_tokens": 100,  # ìµœì†Œ í† í°
    "max_new_tokens": 700,  # ìµœëŒ€ í† í°
    "stop_sequences": ["<|endoftext|>"]  # ì¤‘ì§€ ì‹œí€€ìŠ¤
}

# ğŸ“Œ Watsonx.ai ëª¨ë¸ ì´ˆê¸°í™”
watsonx_model = ModelInference(
    model_id="meta-llama/llama-3-3-70b-instruct",
    credentials=wml_credentials,
    project_id=project_id,
    params=parameters
)

# ğŸ“Œ ChromaDB ì„¤ì •
base_persist_directory = os.path.join(os.path.dirname(__file__), "vectorDB")
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# ë²¡í„°DB ë¡œë“œ
vector_db = Chroma(
    persist_directory=base_persist_directory,
    embedding_function=embedding_model
)

# Retriever ìƒì„± (ìƒìœ„ 3ê°œ ê²€ìƒ‰)
retriever = vector_db.as_retriever(search_kwargs={"k": 3})

# ğŸ“Œ ìš”ì²­ ë°ì´í„° ëª¨ë¸
class QueryRequest(BaseModel):
    prompt: str  # ì‚¬ìš©ì ì§ˆë¬¸
    category: str  # ì„ íƒëœ ì¹´í…Œê³ ë¦¬


def trim_knowledge_base(results, max_tokens=800):
    """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸¸ì´ ì œí•œì— ë§ê²Œ ë‹¤ë“¬ëŠ” í•¨ìˆ˜"""
    knowledge_base = ""
    total_tokens = 0

    for doc in results:
        doc_tokens = len(doc.page_content.split())  # í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜ (ë‹¨ìˆœ ë‹¨ì–´ ìˆ˜ ê¸°ì¤€)
        if total_tokens + doc_tokens > max_tokens:
            break
        knowledge_base += doc.page_content + "\n"
        total_tokens += doc_tokens

    return knowledge_base.strip()


def generate_prompt(results, user_question):
    if not results:
        knowledge_base = "ê´€ë ¨ëœ ì°¸ê³  ìë£Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì•„ë˜ ì§ˆë¬¸ì— ëŒ€í•´ ìµœëŒ€í•œ ëª…í™•íˆ ë‹µë³€í•´ ì£¼ì„¸ìš”."
    else:
        # ì¤‘ë³µ ì œê±° ë° ê¸¸ì´ ì œí•œ ì ìš©
        knowledge_base = trim_knowledge_base(results, max_tokens=800)

    prompt = f"""
-ë‹¹ì‹ ì€ ë³´í˜¸ì¢…ë£Œì•„ë™ì„ ë•ëŠ” **ì¹œì ˆí•œ AI ë¹„ì„œ**ì…ë‹ˆë‹¤.  
-**ê²½ì¾Œí•˜ê³  ì‹ ë‚˜ê³  ìƒëƒ¥í•œ ë§íˆ¬**ë¡œ, "**í•´ìš”ì²´**"ë¡œ ì‘ë‹µí•˜ì„¸ìš”.
-ë°˜ë“œì‹œ ì•„ë˜ ì œê³µëœ **knowledge base**ì˜ ì •ë³´ë§Œì„ ì°¸ê³ í•˜ì—¬ ì‘ë‹µí•˜ì„¸ìš”.

**ì‚¬ìš©ì ì§ˆë¬¸:**  
"{user_question}"

**ë‹¹ì‹ ì´ ì°¸ê³ í•´ì•¼ í•˜ëŠ” ë¬¸ì„œ (Knowledge Base):**  
{knowledge_base}

**[ì´ì œ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”]**
"""
    return prompt

@app.post("/ask/")
def process_question(request: QueryRequest):
    """ì§ˆë¬¸ì— ëŒ€í•œ RAG ì‹œìŠ¤í…œ ì‘ë‹µ ìƒì„± ë° ë””ë²„ê¹…"""
    # ì¹´í…Œê³ ë¦¬ì™€ ì‚¬ìš©ì ì§ˆë¬¸ ì¡°í•©
    full_prompt = f"{request.category} {request.prompt}"
    logger.info(f"âœ… [DEBUG] ìƒì„±ëœ í”„ë¡¬í”„íŠ¸: {full_prompt}")

    # ë²¡í„°DB ê²€ìƒ‰
    results = retriever.invoke(full_prompt)

    if not results:
        logger.warning("âŒ ê²€ìƒ‰ëœ ê´€ë ¨ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return {
            "error": "ê²€ìƒ‰ëœ ê´€ë ¨ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”!"
        }

    # ë””ë²„ê¹…: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¡œê·¸ ì¶œë ¥
    logger.info(f"âœ… [DEBUG] ê²€ìƒ‰ëœ ë¬¸ì„œ (ìµœëŒ€ 3ê°œ):")
    for i, doc in enumerate(results):
        logger.info(f"  ğŸ“„ ë¬¸ì„œ {i + 1}: {doc.page_content[:100]}...")  # ë¬¸ì„œì˜ ì²« 100ì ì¶œë ¥
        logger.info(f"  ğŸ“„ ë¬¸ì„œ {i + 1} ì†ŒìŠ¤: {doc.metadata.get('source', 'ì¶œì²˜ ì—†ìŒ')}")

    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = generate_prompt(results, full_prompt)
    logger.info(f"\nğŸš€ [DEBUG] Watsonx.aiì— ì „ë‹¬ë  ìµœì¢… í”„ë¡¬í”„íŠ¸:\n{prompt}\n{'='*80}")

    # Watsonx.ai í˜¸ì¶œ ë° ê²°ê³¼ ìˆ˜ì§‘
    try:
        response_data = watsonx_model.generate(prompt=prompt)
        answer = response_data["results"][0]["generated_text"].strip()
        logger.info(f"\nâœ… [DEBUG] Watsonx.aiê°€ ìƒì„±í•œ ì‘ë‹µ:\n{answer}\n{'='*80}")
    except Exception as e:
        logger.error(f"âŒ [ERROR] Watsonx.ai ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return {
            "error": "Watsonx.ai ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ",
            "details": str(e)
        }

    # ìµœì¢… ì‘ë‹µ ë°˜í™˜
    logger.info(f"âœ… [DEBUG] ì‚¬ìš©ìì—ê²Œ ì „ë‹¬í•  ìµœì¢… ì‘ë‹µ:\n{answer}")
    return {"answer": answer}
